--------------------------------------------------------------------
AWS Architecture:

AWS (Amazon Web Services) is a cloud computing platform that provides a wide range of services and tools for building, deploying, and managing applications and infrastructure. The architecture of AWS is designed to be highly scalable, flexible, and secure, with a focus on providing customers with the ability to build and run a wide range of applications and services. The core components of the AWS architecture include:

Region and Availability Zone: A geographic area that contains multiple data centers.

EC2 (Elastic Compute Cloud): A scalable computing service that allows users to launch virtual machines.

S3 (Simple Storage Service): A scalable, high-speed, web-based storage service.

RDS (Relational Database Service): A managed relational database service that makes it easy to set up, operate, and scale databases.

VPC (Virtual Private Cloud): A virtual network dedicated to the user's AWS account.

Route 53: A highly available and scalable cloud Domain Name System (DNS) web service.

These are some of the key components of the AWS architecture, and there are many other services available to support a wide range of application and infrastructure needs.
---------------------------------------------------------------------------------------------
AWS VPC:

Amazon Virtual Private Cloud (Amazon VPC) enables you to launch Amazon Web Services (AWS) resources into a virtual network that you've defined. This virtual network closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS.

A VPC allows you to:

Assign custom IP address ranges in each subnet
Configure route tables, network gateways, and security settings
Launch instances in one or more subnets
Control the visibility of instances from the Internet
By launching your resources, such as Amazon Elastic Compute Cloud (EC2) instances, into a VPC, you gain the ability to use Amazon VPC security groups and network ACLs to control inbound and outbound traffic. This provides more secure and scalable deployment options than deploying instances into EC2-Classic.
-----------------------------------------------------------------------------------------------------
Kubernetes Architecture:

Kubernetes is an open-source platform for automating deployment, scaling, and management of containerized applications. The Kubernetes architecture consists of a set of components that work together to provide a platform for deploying, scaling, and operating application containers:

API Server: The API Server is the core component that exposes the Kubernetes API and provides a centralized configuration store for all other components.

etcd: etcd is a distributed key-value store that acts as a data store for the cluster. It is used to store configuration data and cluster state.

Controller Manager: The Controller Manager is responsible for monitoring the state of the cluster and making necessary adjustments to ensure that the desired state is achieved.

Scheduler: The Scheduler is responsible for scheduling containers to run on available nodes in the cluster.

Kubelet: The Kubelet is a small agent that runs on each node in the cluster and is responsible for ensuring that containers are running and healthy.

Container Runtime: The Container Runtime is responsible for running containers on nodes and is typically Docker, but can also be runC, Containerd, or CRI-O.

kubectl: kubectl is the command-line tool used to interact with a Kubernetes cluster.

These components work together to provide a platform for deploying, scaling, and operating containers.
-------------------------------------------------------------------------------------------------------
Git Architecture:

Git is a distributed version control system. In Git, every repository is a complete backup of all the data, including the entire history of all changes. Each local repository contains a full copy of the project, including all versions of all files. The repository contains the entire history of the project, making it possible to go back to a previous version at any time. When changes are made to the code and committed, they are stored in the local repository. Git then provides tools to share and merge changes between different local repositories. The Git architecture provides robustness, efficiency, and decentralization, making it ideal for distributed software development.
----------
Git rebase:
Git rebase is a Git command that allows you to reapply a series of commits to a different base branch. It can be used to clean up a messy commit history, to integrate changes from one branch into another, or to move a series of commits to a new base.

Rebasing works by taking a set of commits, usually from a feature branch, and reapplying them on top of a different branch. This can result in a linear history, where all the changes related to a feature are grouped together.

It is important to note that rebasing changes the commit history and can lead to conflicts if the base branch has been modified since the original branch was created. It is recommended to use rebasing only in local branches and not in shared branches, as it can confuse other team members and cause problems when merging.
--------------
GIT revert:
Git revert is a command that allows you to undo changes in a specific commit. It creates a new commit that undoes the changes made in a previous commit, effectively rolling back the repository to a previous state. The original commit remains in the history of the repository, but its changes are no longer present in the latest version of the code.

The git revert command is useful when you want to undo changes in a specific commit without affecting the rest of the history. It is a safer option than using git reset, which permanently discards commits and their changes.

To revert a specific commit, you need to specify the commit hash or reference to the commit in question. Git will then create a new commit that undoes the changes made in the specified commit, leaving the rest of the history intact.
--------------
Docker architecture:

Docker is a platform that allows developers to easily create, deploy, and run applications in containers. Containers are isolated environments that contain everything an application needs to run, including the code, libraries, system tools, and runtime.

The Docker architecture consists of several components:

Docker Daemon: The Docker daemon is the core component of the Docker platform. It runs on a host machine and manages the creation and lifecycle of containers.

Docker Client: The Docker client is the command-line tool used to interact with the Docker daemon. It allows you to issue commands to the daemon and manage containers.

Docker Image: A Docker image is a pre-built, ready-to-run environment for an application. It includes the code, libraries, tools, and runtime required to run the application.

Docker Registry: A Docker registry is a centralized repository for storing and distributing Docker images. The Docker Hub is the most popular public registry, but private registries can also be used to store and distribute images within an organization.

Docker Container: A Docker container is a standalone executable package that includes everything required to run a piece of software, including the code, a runtime, libraries, environment variables, and config files.

Overall, the Docker architecture provides a simple, efficient, and scalable way to package and distribute applications. The use of containers makes it easy to run the same application on any host, ensuring consistency and reducing the overhead of managing dependencies and configuration.
------------------------
Docker File:

A Dockerfile is a script that contains instructions for building a Docker image. It defines the environment and components required to run a specific application, including the base image, dependencies, files, and config files.

Each line in a Dockerfile corresponds to a command that is executed during the build process. The Dockerfile starts with a base image, which provides a foundation for the rest of the build. Then, various commands are used to add files, install dependencies, and configure the environment.

A Dockerfile typically includes the following instructions:

FROM: Specifies the base image to use as a starting point.

RUN: Executes shell commands to install dependencies and configure the environment.

COPY: Copies files from the host machine into the image.

WORKDIR: Specifies the working directory for the subsequent instructions.

ENV: Sets environment variables for the image.

EXPOSE: Specifies the ports that the application listens on.

CMD: Specifies the command to run when a container is started from the image.

The Dockerfile is used to build a Docker image, which can then be used to start containers. By using a Dockerfile, you can automate the build process and ensure that the image is consistently built every time. This makes it easier to deploy and run applications in containers.
-------------------------
Diff between COPY and ADD in docker file

COPY and ADD are both instructions in a Dockerfile used to copy files from the host machine into a Docker image. However, there are some differences between the two:

COPY: The COPY instruction is used to copy files from the host machine into the Docker image file system. It only supports local files and does not support remote URLs or archive extraction. It is a simple and straightforward instruction for copying files.

ADD: The ADD instruction is more powerful than COPY and supports both local files and remote URLs. It also supports archive extraction, which means it can automatically unpack .tar and .zip files. However, the use of remote URLs and archive extraction can make the build process more complex and harder to understand.
---------------------------


In general, it is recommended to use COPY when copying files from the host machine into the Docker image, as it is simpler and less prone to errors. Use ADD only when you need the additional functionality provided, such as archive extraction or downloading files from a remote URL.
